
# ğŸŒ¾ KCC Query Assistant (Offline + Fallback)

This is a local-first AI application that allows users to query agricultural advice from the **Kisan Call Center (KCC)** dataset using a local LLM (Gemma 2B via Ollama) and semantic search (FAISS). If no local context is found, it gracefully falls back to live internet search using SerpAPI.

---

## ğŸ”§ Features

- âœ… Offline semantic search with FAISS and HuggingFace Embeddings
- âœ… Local LLM via Ollama (Gemma 2B)
- âœ… Preprocessing of real agricultural Q&A data (KCC)
- âœ… Metadata-preserved document chunking
- âœ… Streamlit UI with structured display
- âœ… Fallback to SerpAPI search if no relevant local context is found

---

## ğŸ› ï¸ Installation

```bash
git clone https://github.com/nageswarao7/NageswaraRaoVutla_KCCQueryAssistant.git
cd KCCQueryAssistant
pip install -r requirements.txt
```

---

## â–¶ï¸ How to Run

1. **Start Ollama and ensure Gemma is installed:**

   ```bash
   ollama run gemma:2b
   ```

2. **Run Streamlit app:**

   ```bash
   streamlit run app.py
   ```

---

## ğŸ” Streamlit Secrets

Create a file named `.streamlit/secrets.toml`:

```toml
SERPAPI_API_KEY = "your_serpapi_api_key"
```

---

## ğŸ“ Project Structure

```
â”œâ”€â”€ app.py                    # Streamlit UI
â”œâ”€â”€ utils.py                  # Preprocessing, FAISS, and LLM functions
â”œâ”€â”€ sampleeeee.csv            # Raw dataset
â”œâ”€â”€ processed_docs.json       # Preprocessed Q&A chunks
â”œâ”€â”€ faiss_index.pkl           # FAISS index
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .streamlit/
â”‚   â””â”€â”€ secrets.toml          # Your SerpAPI key
â””â”€â”€ README.md
```
